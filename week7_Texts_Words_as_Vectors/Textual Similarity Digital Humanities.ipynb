{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual similarity\n",
    "\n",
    "We'll start by importing some things we will need later on.  Seaborn is a package that makes Matplotlib look nicer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib seaborn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "import math\n",
    "from matplotlib.colors import LogNorm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document vectors\n",
    "\n",
    "We need a way to \"vectorize\" the text - i.e. convert each sample from a string of characters into some (fixed) number of features. We'll be doing this a lot, so we will go through it carefully.\n",
    "\n",
    "To start, let's suppose we have a short list of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\"This is the text of the first document\", \n",
    "             \"This is the second one\", \n",
    "             \"Some other document\", \n",
    "             \"The next one\", \n",
    "             \"Another document - the last document in our list of documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to quickly tokenize a string into a list of words is to use nltk's word_tokenize function.  For now we won't do any lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_texts[0])\n",
    "print(word_tokenize(raw_texts[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to be treating documents as sequences of tokens, we can start by turning each document into a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_documents(raw):\n",
    "    docs = []\n",
    "    for text in raw:\n",
    "        docs.append([word.lower() for word in word_tokenize(text)])\n",
    "    return docs\n",
    "\n",
    "documents = text_to_documents(raw_texts)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do any vectorization, we're going to need to know what all the unique tokens are. For convenience, we define a function that takes a list of lists (i.e. a list of documents), and returns the unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_terms(docs):\n",
    "    words = {}\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            words[word] = True\n",
    "    return list(words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. the unique terms in just the *first* document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms([documents[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're really interested in however is the list of unique terms across *all* documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = unique_terms(documents)\n",
    "print(terms)\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can create a list of sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [set(doc) for doc in documents]\n",
    "sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then make a set that is the union of all of the sets in the list. This uses more advanced Python syntax (the asterisk is a list expansion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms2 = set.union(*sets)\n",
    "terms2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of terms, we can use this to vectorize a document. The order of information in our vectors will be determined by the order of our term list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_tf_vector(doc, terms):\n",
    "    vector = []\n",
    "    for term in terms:\n",
    "        vector.append(doc.count(term))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this first for a single document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "document_tf_vector(documents[0], terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to turn all of our documents into TF vectors; if we turn this into a Pandas dataframe, we can also add our term list as column headings to better see what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_tf_vectors(docs, terms):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vectors.append(document_tf_vector(doc, terms))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = document_tf_vectors(documents, terms)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors, columns=terms)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have vectors, we can start comparing them; let's define cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    assert len(vector1) == len(vector2)\n",
    "    product = 0\n",
    "    norm1squared = 0\n",
    "    norm2squared = 0\n",
    "    for value1, value2 in zip(vector1, vector2):\n",
    "        product += value1*value2\n",
    "        norm1squared += value1**2\n",
    "        norm2squared += value2**2\n",
    "    return product/(math.sqrt(norm1squared * norm2squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How similar is vector 0 (document 0) to vector 1 (document 1)?\n",
    "print(cosine_similarity(vectors[0], vectors[1]))\n",
    "\n",
    "# How similar is vector 3 (document 3) to vector 3 (document 3)?\n",
    "print(cosine_similarity(vectors[3], vectors[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we're still using just term frequency, this is a good point to start looking at how we might make a global comparison among all of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_vectors(vectors):\n",
    "    matrix = []\n",
    "    for vector_i in vectors:\n",
    "        row = []\n",
    "        for vector_j in vectors:\n",
    "            row.append(cosine_similarity(vector_i, vector_j))\n",
    "        matrix.append(row)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = compare_all_vectors(vectors)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix isn't that easy to read directly, but it corresponds to an intuitive heatmap - let's define a helper function for that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_heatmap(vectors):\n",
    "    matrix = compare_all_vectors(vectors)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(matrix, square=True, annot=True, cmap='Reds', cbar=True)\n",
    "    plt.xlabel('Document vector #')\n",
    "    plt.ylabel('Document vector #');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    compare_all_heatmap(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Which two documents in this simple example are the most similar according to cosine similarity on TF vectors? With reference to the contents of the documents, *why* do we get this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we defined each individual step as a function, we can repeat things quite easily with a different corpus - let's use a very simple one with two terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\"cat cat cat cat mouse\", \\\n",
    "             \"cat dog\", \\\n",
    "             \"dog dog cat dog\", \\\n",
    "             \"dog dog cat cat\", \\\n",
    "             \"cat cat cat dog\", \\\n",
    "             \"dog dog dog dog mouse\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_to_documents(raw_texts)\n",
    "terms = unique_terms(documents)\n",
    "vectors = document_tf_vectors(documents, terms)\n",
    "compare_all_heatmap(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:**  Why are there two cells off the diagonal with one in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see similarity comparisons, we can implement Inverse Document Frequency weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(term, documents):\n",
    "    count = 0\n",
    "    for doc in documents:\n",
    "        if term in doc:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term, documents):\n",
    "    n = len(documents)\n",
    "    return math.log(n/df(term, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the document frequency and inverse document frequency of any term in our set of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df(\"dog\", documents))\n",
    "print(idf(\"dog\", documents))\n",
    "\n",
    "print(df(\"mouse\", documents))\n",
    "print(idf(\"mouse\", documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to construct document vectors, this time using TF-IDF instead of just TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_tfidf_vector(doc, terms, documents):\n",
    "    vector = []\n",
    "    for term in terms:\n",
    "        vector.append(doc.count(term) * idf(term, documents))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_tfidf_vectors(docs, terms):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vectors.append(document_tfidf_vector(doc, terms, docs))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have this, we can construct a comparison matrix, and plot it as a heatmap exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = document_tfidf_vectors(documents, terms)\n",
    "compare_all_heatmap(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Compare the output for this example (cats+dogs, TF-IDF) with the earlier output (cats+dogs, TF). Are there any differences? Explain why we get this result with this corpus (hint: think about the IDF values for each of the terms in this very small corpus).\n",
    "\n",
    "**Q:** Our corpus was intentionally constructed with just two unique terms. What happens if you add a third term to one of the documents (e.g. mouse to the last one)?\n",
    "\n",
    "**Q:** What happens if you additionally add another, different term to another of the documents (e.g. rat to the first one)? Which of the two charts (TF, or TF-IDF) is more radically altered by these changes? Why is this?  \n",
    "\n",
    "**Q:** What if you add the first new term to the second document instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real corpus\n",
    "\n",
    "So far we've just played around with very short documents to better understand what's going on. In this section, we'll load more realistic documents from text files instead.\n",
    "\n",
    "**Note:** before running this code, you will need to unzip the text files and put the two folders in the same folder as this notebook.\n",
    "\n",
    "So you should have a folder named `alice1` and another named `alice2`.\n",
    "\n",
    "We define a helper function to read all the `.txt` files in a specified folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory(folder):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            labels.append(filename)\n",
    "    labels.sort()\n",
    "    for filename in labels:\n",
    "        with open(os.path.join(folder, filename), encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return [texts, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first example text is a copy of Alice in Wonderland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts, labels = read_directory(\"alice1\")\n",
    "raw_texts[1][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function also gives us the filenames so we can keep track of which file is which document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined all the steps above as functions, so we can re-run the same steps on our new corpus easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_to_documents(raw_texts)\n",
    "terms = unique_terms(documents)\n",
    "vectors = document_tfidf_vectors(documents, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_heatmap(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load  a separate text - the manuscript version \"Alice's adventures underground\" (\"alice2\" folder), which differs substantially from the published 12 chapter version of the text (\"alice1\" folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts2, labels2 = read_directory(\"alice2\")\n",
    "documents = text_to_documents(raw_texts + raw_texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = unique_terms(documents)\n",
    "vectors = document_tfidf_vectors(documents, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_heatmap(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some interesting similarities between the two versions of Alice in Wonderland.\n",
    "\n",
    "* Using the heatmap, which chapters of the 12-chapter edition (files in the \"alice1\" folder) likely correspond to the first chapter of the 4-chapter edition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "We will use the `ngrams` function from NLTK to extract n-grams from our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\"This is the text of the first document\", \\\n",
    "             \"This is the second one\", \\\n",
    "             \"Some other document\", \\\n",
    "             \"The next one\", \\\n",
    "             \"Another document - the last document in our list of documents\"]\n",
    "documents = text_to_documents(raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll define a variable \"N\" and use that as the \"n\" in \"n-gram\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = list(ngrams(documents[0], N))\n",
    "ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to define the Jaccard index will be to make use of Python's set functions. Unlike a list, a set does not allow duplicated members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([1,2,2,2,2,3,4])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = set([2,3,4,5,5,5,5])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a convenient way to define Jaccard index, because Python has operators for intersection (&) and union (|) of sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a&b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a|b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using these operators, our definition becomes very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    if len(a|b) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0 * len(a&b)/len(a|b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to compare the similarity of lists of ngrams, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng1 = list(ngrams(documents[0], N))\n",
    "ng2 = list(ngrams(documents[1], N))\n",
    "jaccard_index(ng1, ng2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can also define a function that builds a list of ngrams for each of our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams(documents, N):\n",
    "    ngram_list = []\n",
    "    for doc in documents:\n",
    "        ngram_list.append(list(ngrams(doc, N)))\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_list = build_ngrams(documents, N)\n",
    "ngram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with cosine similarity, a matrix containing all comparisons will help give us a good overview of all possible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_ngrams(ngram_list):\n",
    "    matrix = []\n",
    "    for ngram_i in ngram_list:\n",
    "        row = []\n",
    "        for ngram_j in ngram_list:\n",
    "            row.append(jaccard_index(ngram_i, ngram_j))\n",
    "        matrix.append(row)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_ngrams(ngram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, this matrix is going to be much easier to look at as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_ngrams_heatmap(ngram_list):\n",
    "    matrix = compare_all_ngrams(ngram_list)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(matrix, square=True, annot=True, cmap='Reds', cbar=True, norm=LogNorm())\n",
    "    plt.xlabel('Document vector')\n",
    "    plt.ylabel('Document vector');\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_ngrams_heatmap(ngram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the set operators to examine which specific ngrams are shared between any two documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ngram_list[0]) & set(ngram_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are shared n-grams, there isn't a huge amount of obvious reuse in our current corpus.  Let's try the two versions of Alice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "raw_texts, labels = read_directory(\"alice1\")\n",
    "raw_texts2, labels2 = read_directory(\"alice2\")\n",
    "documents = text_to_documents(raw_texts + raw_texts2)\n",
    "ngram_list = build_ngrams(documents, N)\n",
    "compare_all_ngrams_heatmap(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ngram_list), len(ngram_list[0]))\n",
    "ngram_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How dependent are the general trends visible in the heatmap on the chosen value of N? Experiment with different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation\n",
    "\n",
    "Our implementation of TF-IDF and cosine similarity is very slow with large documents, because it's intentionally written to favour readability over performance. \n",
    "\n",
    "In the future, we will use the **much** more efficient implementation that is included in the sklearn library as \"[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\". You can experiment with this library, and compare the results with those from the code above.\n",
    "\n",
    "As we will see, there are minor differences between the sklearn implementation and the standard version of TF-IDF described in class and implemented in this notebook. See the sklearn documentation for the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None,sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(raw_texts + raw_texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_heatmap(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (for ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), lowercase=True)\n",
    "counts = vectorizer.fit_transform(raw_texts + raw_texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorizer.get_feature_names_out()))\n",
    "vectorizer.get_feature_names_out()[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have counts of ngrams rather than lists of ngrams, so we use the original heatmap function, which uses cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_heatmap(counts.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
